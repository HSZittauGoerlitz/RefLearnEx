{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import logging\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly import graph_objects as go\n",
    "import shap\n",
    "import torch\n",
    "# Custom\n",
    "from cartpole import CartPoleRegulatorEnv\n",
    "from NFQagents import NFQagent, ReplayMemory, Transition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Set up Logger\n",
    "logger = logging.getLogger('lastRun')\n",
    "logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(\"lastRun.log\", 'w')\n",
    "fh.setFormatter(logging.Formatter(\n",
    "    '[%(levelname)s] - %(asctime)s - %(message)s'))\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-System\n",
    "\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24 deg        24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        \n",
    "        Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selbstlernender Regler mittels _NFQ-Learning_ / _DQN-Learning_\n",
    "Der selbstlernende Regler wird mittels Neuro-Fitted-Q-Learning Realisiert. Die Implementierung erfolgt in Anlehnung an die Dissertation von Dr. Hafner [1] sowie an die Veröffentlichung von Prof. Riedmiller [2].\n",
    "\n",
    "Das klassische NFQ-Verfahren ist ein offline Lernverfahren, bei dem ein Multi-Layer-Perceptron wiederholt auf die gleiche Datenbasis trainiert wird. Ziel dabei ist es zu lernen, welche Aktion im jeweiligen Zustand den größten langfristigen Erfolg verspricht (Q-Funktion). Die Kosten/Belohnungen, welche Teil des Trainingsdatensatzes sind dürfen dabei nicht kumuliert werden, sondern beziehen sich immer auf die aktuell gewählte Aktion.\n",
    "\n",
    "## Q-Funktion am _Cartpole_ Beispiel\n",
    "Gäbe es für ein gegebenes System eine Zuordnung aus allen möglichen Zuständen sowie entstehenden Kosten (Minimierungsaufgabe) bzw. den entstehenden Gewinn (Maximierungsaufgabe) zu jeder möglichen Aktion, wäre es möglich dieses System optimal zu steuern. Wäre eine solche Zuordnung beispielsweise für das Spiel Schach bekannt, wäre es möglich jedes Spiel zu gewinnen, da in jedem Zug die Optimale Entscheidung getroffen werden kann. Leider ist bereits für einfache Systeme, wie dem Schach-Beispiel, diese Zuordnung zu umfangreich, um diese sinnvoll zu speichern oder sogar anzuwenden. Daher wurden Verfahren Entwickelt um diese Zuordnung zu Approximieren. Dabei erfolgt die Approximation durch eine *Q-Funktion*. Beim NFQ-Verfahren wird die Q-Funktion durch ein vorwärtsgerichtetes künstliches Neuronales Netz umgesetzt. Dies sieht für das Cartpole-Beispiel so aus:\n",
    "\n",
    "![Neuronales Netz zur Approximation der Q-Funktion im Cartpole-Bsp.](QfunNet_Cartpole.svg)\n",
    "\n",
    "## Konvergenzverhalten\n",
    "Durch das iterative Trainingsverfahren läuft das Training nicht stabil ab. Dadurch wird keine kontinuierliche Verbesserung erreicht, sondern es kann vorkommen, dass ein bereits guter Regler wieder schlechter wird. In [1] werden diese diese Effekte näher erläutert und Methoden umd diese abzumildern beschrieben. Im vorliegenden Beispiel sind diese Methoden bereits berücksichtigt.\n",
    "\n",
    "## Erweiterung\n",
    "In Anlehnung an das neuere Deep-Q-Learning wurde für das NFQ Verfahren eine Online-Komponente implementiert. Das heißt, der Speicher welcher zum Training des neuronalen Netzes verwendet wird, wird zur Trainingszeit durch aktuelle Erfahrungswerte erweitert. Zusätzlich besteht die Möglichkeit das KNN, welches die Systemsteuerung übernimmt von dem KNN, welches die Zielwerte für das Training berechnet zu trennen. Das Zielnetz wird dabei nur in größeren Abständen auf das kontinuierlich Trainierte Hauptnetz angepasst. Dies führt zu einer besseren Konvergenz des Lernalgorithmus.\n",
    "\n",
    "## Literatur\n",
    "[1]R. Hafner, „Dateneffiziente selbstlernende neuronale Regler“, Universität Osnabrück, Osnabrück, 2009.\n",
    "\n",
    "[2]M. Riedmiller, „Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method“, in Machine Learning: ECML 2005, Bd. 3720, J. Gama, R. Camacho, P. B. Brazdil, A. M. Jorge, und L. Torgo, Hrsg. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, S. 317–328. doi: 10.1007/11564096_32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent mit Erfahrungsspeicher und -modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = True\n",
    "renderStep = 10\n",
    "shapStep = 101\n",
    "shapSamples = 100\n",
    "# hyper parameter\n",
    "nEpoch = 800\n",
    "\n",
    "nBatch = 800\n",
    "nEval = 3  # repetitions per evaluation step\n",
    "\n",
    "memCapacity = 16000 # number of transitions to store in experience memory\n",
    "eps0 = 0.3\n",
    "\n",
    "# max transition cost\n",
    "cMax = 1.\n",
    "cFix = 0.  # small reward for each successfull transition\n",
    "cStep = 0.016  # see \"viewCostFunction.py\"\n",
    "\n",
    "train_env = CartPoleRegulatorEnv(cMax, cFix, mode=\"train\")\n",
    "eval_env = CartPoleRegulatorEnv(cMax, cFix, mode=\"eval\")\n",
    "\n",
    "# init agent\n",
    "# slightly over the max. possible costs, \n",
    "# to prevent max. values in sigmoid activation of output layer\n",
    "nSteps = max(train_env.max_steps, eval_env.max_steps)\n",
    "QbMin =  -nSteps * cStep + nSteps * cFix  # - nSteps * max cost of one step\n",
    "QbMax =  cMax + nSteps * cStep + nSteps * cFix  # cost on fail + nSteps * max cost of one step\n",
    "agent = NFQagent(4, 2, QbMin, QbMax, eps0)\n",
    "memory = ReplayMemory(memCapacity)\n",
    "\n",
    "# define helper functions\n",
    "def evaluate(agent, env, memory=None, render=False):\n",
    "    # switch of exploration, if no memory update\n",
    "    if memory is None:\n",
    "        agent.Epsilon = 0.\n",
    "    state = env.reset()\n",
    "    costs = 0.\n",
    "\n",
    "    for stepIdx in range(env.max_steps):\n",
    "        if memory:\n",
    "            state_old = state.copy()\n",
    "        aIdx = agent.act(state)\n",
    "        state, cost, done, info = env.step(aIdx)\n",
    "\n",
    "        if memory:\n",
    "            memory.push(torch.FloatTensor(state_old),\n",
    "                        aIdx, \n",
    "                        torch.FloatTensor(state),\n",
    "                        cost, float(done))\n",
    "        costs += cost\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    return costs, stepIdx\n",
    "\n",
    "def fillMemoryExplore(nActions, memory, agent, train_env):\n",
    "    fills = 0\n",
    "    while fills < memory.capacity:\n",
    "        aIdx = np.random.randint(nActions)\n",
    "        state = train_env.reset()\n",
    "        for _ in range(nBatch):\n",
    "            aIdx = agent.act(state)\n",
    "            next_state, cost, done, info = train_env.step(aIdx)\n",
    "            if done:\n",
    "                break\n",
    "            memory.push(torch.FloatTensor(state), \n",
    "                        aIdx, \n",
    "                        torch.FloatTensor(next_state),\n",
    "                        cost, 0.)\n",
    "            fills += 1\n",
    "            state = np.copy(next_state)\n",
    "\n",
    "        memory.push(torch.FloatTensor(state), \n",
    "                    aIdx, \n",
    "                    torch.FloatTensor(next_state),\n",
    "                    cost, float(done))\n",
    "        fills += 1\n",
    "\n",
    "def callNN(data):\n",
    "    \"\"\" Helper function for shap value calculation \n",
    "        The shap package only accepts numpy arrays. Hence a wrapper around\n",
    "        the torch tensors is needed.\n",
    "    \"\"\"\n",
    "    return agent.model(torch.from_numpy(data).float()).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Visualisation\n",
    "# save and show training loss of each epoch\n",
    "xEpochs = np.arange(nEpoch)\n",
    "loss = np.zeros(nEpoch)\n",
    "evalSucc = np.zeros(nEpoch)\n",
    "cost = np.zeros(nEpoch)\n",
    "cMean = np.zeros(nEpoch-50)\n",
    "cStdU = np.zeros(nEpoch-50)\n",
    "cStdL = np.zeros(nEpoch-50)\n",
    "# observe agents min and max Q-Values\n",
    "Qmin = np.zeros(nEpoch)\n",
    "Qmax = np.zeros(nEpoch)\n",
    "# shap values (influence of nn inputs to outputs)\n",
    "xShap = xEpochs[::shapStep]\n",
    "sPositionL = np.zeros(xShap.size)\n",
    "sVelocityL = np.zeros(xShap.size)\n",
    "sAngleL = np.zeros(xShap.size)\n",
    "sTipVelocityL = np.zeros(xShap.size)\n",
    "sPositionR = np.zeros(xShap.size)\n",
    "sVelocityR = np.zeros(xShap.size)\n",
    "sAngleR = np.zeros(xShap.size)\n",
    "sTipVelocityR = np.zeros(xShap.size)\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    vertical_spacing=0.02,\n",
    "                    specs=[[{'secondary_y': True}], [{'secondary_y': True}], \n",
    "                           [{'secondary_y': False}]])\n",
    "fig.update_xaxes(title_text=\"number of Epoch\", row=3, col=1)\n",
    "fig.update_yaxes(row=1, col=1, title_text=\"Cost\", autorange=True)\n",
    "fig.update_yaxes(row=1, col=1, title_text=\"Evaluation Success\", \n",
    "                 range=[0., nEval], secondary_y=True, showgrid=False,\n",
    "                 autorange=False)\n",
    "fig['layout']['yaxis2']['showgrid'] = False\n",
    "fig.update_yaxes(row=2, col=1, title_text=\"Q Values\", autorange=True,\n",
    "                 range=[QbMin, QbMax])\n",
    "fig.update_yaxes(row=2, col=1, title_text=\"ANN Loss\", autorange=True, \n",
    "                 secondary_y=True, showgrid=False)\n",
    "fig['layout']['yaxis4']['showgrid'] = False\n",
    "fig.update_yaxes(row=3, col=1, title_text=\"Shap Value\", autorange=True)\n",
    "\n",
    "lineCost = go.Scatter({\"x\": xEpochs,\n",
    "                       \"y\": cost,\n",
    "                       \"opacity\": 0.25,\n",
    "                       \"name\": \"cost\",\n",
    "                       \"uid\": \"uid_lineCost\",\n",
    "                       \"yaxis\": \"y1\",\n",
    "                       \"line\": {\"color\": \"#000000\",\n",
    "                                \"width\": 1\n",
    "                               }\n",
    "                       })\n",
    "lineCostM = go.Scatter({\"x\": xEpochs[50:],\n",
    "                        \"y\": cMean,\n",
    "                        \"name\": \"cost mean\",\n",
    "                        \"uid\": \"uid_lineCostM\",\n",
    "                        \"yaxis\": \"y1\",\n",
    "                        \"line\": {\"color\": \"#000000\",\n",
    "                                 \"width\": 1\n",
    "                                }\n",
    "                        })\n",
    "lineCostSU = go.Scatter({\"x\": xEpochs[50:],\n",
    "                         \"name\": \"cost\",\n",
    "                         \"y\": cStdU,\n",
    "                         \"uid\": \"uid_lineCostSU\",\n",
    "                         \"yaxis\": \"y1\",\n",
    "                         \"line\": {\"color\": \"#000000\",\n",
    "                                  \"width\": 0.5,\n",
    "                                  \"dash\": \"dot\"\n",
    "                                 },\n",
    "                         \"showlegend\": False\n",
    "                         })\n",
    "lineCostSL = go.Scatter({\"x\": xEpochs[50:],\n",
    "                         \"y\": cStdL,\n",
    "                         \"name\": \"cost\",\n",
    "                         \"uid\": \"uid_lineCostSL\",\n",
    "                         \"yaxis\": \"y1\",\n",
    "                         \"line\": {\"color\": \"#000000\",\n",
    "                                  \"width\": 0.5,\n",
    "                                  \"dash\": \"dot\"\n",
    "                                 },\n",
    "                         \"showlegend\": False\n",
    "                         })\n",
    "lineEval = go.Scatter({\"x\": xEpochs,\n",
    "                       \"y\": evalSucc,\n",
    "                       \"name\": \"evalSucc\",\n",
    "                       \"uid\": \"uid_lineEval\",\n",
    "                       \"yaxis\": \"y2\",\n",
    "                       \"line\": {\"color\": \"#9CCC66\",\n",
    "                                \"width\": 2\n",
    "                               }\n",
    "                       })\n",
    "lineQmin = go.Scatter({\"x\": xEpochs,\n",
    "                       \"y\": Qmin,\n",
    "                       \"name\": \"Qmin\",\n",
    "                       \"uid\": \"uid_lineQmin\",\n",
    "                       \"yaxis\": \"y1\",\n",
    "                       \"line\": {\"color\": \"#CC4F4F\",\n",
    "                                \"width\": 1\n",
    "                                }\n",
    "                       })\n",
    "lineQmax = go.Scatter({\"x\": xEpochs,\n",
    "                       \"y\": Qmax,\n",
    "                       \"name\": \"Qmax\",\n",
    "                       \"uid\": \"uid_lineQmax\",\n",
    "                       \"yaxis\": \"y1\",\n",
    "                       \"line\": {\"color\": \"#1E70CC\",\n",
    "                                \"width\": 1\n",
    "                                }\n",
    "                       })\n",
    "lineLoss = go.Scatter({\"x\": xEpochs,\n",
    "                       \"y\": loss,\n",
    "                       \"name\": \"loss\",\n",
    "                       \"uid\": \"uid_lineLoss\",\n",
    "                       \"yaxis\": \"y2\",\n",
    "                       \"line\": {\"color\": \"#D83C20\",\n",
    "                                \"width\": 1,\n",
    "                                \"dash\": \"dot\"\n",
    "                               }\n",
    "                       })\n",
    "line_sPositionL = go.Scatter({\"x\": xShap,\n",
    "                              \"y\": sPositionL,\n",
    "                              \"name\": \"Position->Left\",\n",
    "                              \"uid\": \"uid_line_sPositionL\",\n",
    "                              \"yaxis\": \"y3\",\n",
    "                              \"line\": {\"color\": \"#047de7\",\n",
    "                                       \"width\": 1,\n",
    "                                       }},\n",
    "                              legendgroup='Shap Values',\n",
    "                              legendgrouptitle_text='Shap Values'\n",
    "                              )\n",
    "line_sVelocityL = go.Scatter({\"x\": xShap,\n",
    "                              \"y\": sVelocityL,\n",
    "                              \"name\": \"Velocity->Left\",\n",
    "                              \"uid\": \"uid_line_sVelocityL\",\n",
    "                              \"yaxis\": \"y3\",\n",
    "                              \"line\": {\"color\": \"#2d3ab4\",\n",
    "                                       \"width\": 1,\n",
    "                                       }},\n",
    "                              legendgroup='Shap Values',\n",
    "                              legendgrouptitle_text='Shap Values'\n",
    "                              )\n",
    "line_sAngleL = go.Scatter({\"x\": xShap,\n",
    "                           \"y\": sAngleL,\n",
    "                           \"name\": \"Angle->Left\",\n",
    "                           \"uid\": \"uid_line_sAngleL\",\n",
    "                           \"yaxis\": \"y3\",\n",
    "                           \"line\": {\"color\": \"#7ec71f\",\n",
    "                                    \"width\": 1,\n",
    "                                    }},\n",
    "                           legendgroup='Shap Values',\n",
    "                           legendgrouptitle_text='Shap Values'\n",
    "                           )\n",
    "line_sTipVelocityL = go.Scatter({\"x\": xShap,\n",
    "                                 \"y\": sTipVelocityL,\n",
    "                                 \"name\": \"TipVelocity->Left\",\n",
    "                                 \"uid\": \"uid_line_sTipVelocityL\",\n",
    "                                 \"yaxis\": \"y3\",\n",
    "                                 \"line\": {\"color\": \"#ada505\",\n",
    "                                          \"width\": 1,\n",
    "                                         }},\n",
    "                                 legendgroup='Shap Values',\n",
    "                                 legendgrouptitle_text='Shap Values'\n",
    "                                 )\n",
    "line_sPositionR = go.Scatter({\"x\": xShap,\n",
    "                              \"y\": sPositionR,\n",
    "                              \"name\": \"Position->Right\",\n",
    "                              \"uid\": \"uid_line_sPositionR\",\n",
    "                              \"yaxis\": \"y3\",\n",
    "                              \"line\": {\"color\": \"#047de7\",\n",
    "                                       \"width\": 1,\n",
    "                                       \"dash\": \"dot\"\n",
    "                                       }},\n",
    "                              legendgroup='Shap Values',\n",
    "                              legendgrouptitle_text='Shap Values'\n",
    "                              )\n",
    "line_sVelocityR = go.Scatter({\"x\": xShap,\n",
    "                              \"y\": sVelocityR,\n",
    "                              \"name\": \"Velocity->Right\",\n",
    "                              \"uid\": \"uid_line_sVelocityR\",\n",
    "                              \"yaxis\": \"y3\",\n",
    "                              \"line\": {\"color\": \"#2d3ab4\",\n",
    "                                       \"width\": 1,\n",
    "                                       \"dash\": \"dot\"\n",
    "                                       }},\n",
    "                              legendgroup='Shap Values',\n",
    "                              legendgrouptitle_text='Shap Values'\n",
    "                              )\n",
    "line_sAngleR = go.Scatter({\"x\": xShap,\n",
    "                           \"y\": sAngleR,\n",
    "                           \"name\": \"Angle->Right\",\n",
    "                           \"uid\": \"uid_line_sAngleR\",\n",
    "                           \"yaxis\": \"y3\",\n",
    "                           \"line\": {\"color\": \"#7ec71f\",\n",
    "                                    \"width\": 1,\n",
    "                                    \"dash\": \"dot\"\n",
    "                                    }},\n",
    "                           legendgroup='Shap Values',\n",
    "                           legendgrouptitle_text='Shap Values'\n",
    "                           )\n",
    "line_sTipVelocityR = go.Scatter({\"x\": xShap,\n",
    "                                 \"y\": sTipVelocityR,\n",
    "                                 \"name\": \"TipVelocity->Right\",\n",
    "                                 \"uid\": \"uid_line_sTipVelocityR\",\n",
    "                                 \"yaxis\": \"y3\",\n",
    "                                 \"line\": {\"color\": \"#ada505\",\n",
    "                                          \"width\": 1,\n",
    "                                          \"dash\": \"dot\"\n",
    "                                         }},\n",
    "                                 legendgroup='Shap Values',\n",
    "                                 legendgrouptitle_text='Shap Values'\n",
    "                                 )\n",
    "fig.update_layout(title='', height=800)\n",
    "fig.add_trace(lineCost, row=1, col=1)\n",
    "fig.add_trace(lineCostM, row=1, col=1)\n",
    "fig.add_trace(lineCostSU, row=1, col=1)\n",
    "fig.add_trace(lineCostSL, row=1, col=1)\n",
    "fig.add_trace(lineEval, row=1, col=1, secondary_y=True)\n",
    "fig.add_trace(lineQmin, row=2, col=1)\n",
    "fig.add_trace(lineQmax, row=2, col=1)\n",
    "fig.add_trace(lineLoss, row=2, col=1, secondary_y=True)\n",
    "fig.add_trace(line_sPositionL, row=3, col=1)\n",
    "fig.add_trace(line_sVelocityL, row=3, col=1)\n",
    "fig.add_trace(line_sAngleL, row=3, col=1)\n",
    "fig.add_trace(line_sTipVelocityL, row=3, col=1)\n",
    "fig.add_trace(line_sPositionR, row=3, col=1)\n",
    "fig.add_trace(line_sVelocityR, row=3, col=1)\n",
    "fig.add_trace(line_sAngleR, row=3, col=1)\n",
    "fig.add_trace(line_sTipVelocityR, row=3, col=1)\n",
    "cEndWidget = go.FigureWidget(fig)\n",
    "# get direct connection to line data (overwrite variables used for lines)\n",
    "lineCost = cEndWidget.data[0]\n",
    "lineCostM = cEndWidget.data[1]\n",
    "lineCostSU = cEndWidget.data[2]\n",
    "lineCostSL = cEndWidget.data[3]\n",
    "lineEval = cEndWidget.data[4]\n",
    "lineQmin = cEndWidget.data[5]\n",
    "lineQmax = cEndWidget.data[6]\n",
    "lineLoss = cEndWidget.data[7]\n",
    "line_sPositionL = cEndWidget.data[8]\n",
    "line_sVelocityL = cEndWidget.data[9]\n",
    "line_sAngleL = cEndWidget.data[10]\n",
    "line_sTipVelocityL = cEndWidget.data[11]\n",
    "line_sPositionR = cEndWidget.data[12]\n",
    "line_sVelocityR = cEndWidget.data[13]\n",
    "line_sAngleR = cEndWidget.data[14]\n",
    "line_sTipVelocityR = cEndWidget.data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cEndWidget)\n",
    "\n",
    "# RUN TRAINING\n",
    "train = True\n",
    "while train:\n",
    "    # get data\n",
    "    agent.Epsilion = 1.\n",
    "    fillMemoryExplore(2, memory, agent, train_env)\n",
    "    agent.Epsilion = eps0\n",
    "    agent.std = np.array([4.8, 10., 0.5*np.pi, 10.], dtype=np.float32)\n",
    "    memory.updateCapacity(int(memCapacity*2))\n",
    "\n",
    "    # reinit vis arrays\n",
    "    cost *= 0.\n",
    "    cMean *= 0.\n",
    "    cStdU *= 0.\n",
    "    cStdL *= 0.\n",
    "    evalSucc *= 0\n",
    "    Qmin *= 0.\n",
    "    Qmax *= 0.\n",
    "    loss *= 0.\n",
    "    sPositionL *= 0.\n",
    "    sVelocityL *= 0.\n",
    "    sAngleL *= 0.\n",
    "    sTipVelocityL *= 0.\n",
    "    sPositionR *= 0.\n",
    "    sVelocityR *= 0.\n",
    "    sAngleR *= 0.\n",
    "    sTipVelocityR *= 0.\n",
    "    # Index for shap value curves\n",
    "    sIdx = 0\n",
    "\n",
    "    for epoch in range(nEpoch):\n",
    "        l, Qs = agent.batchTrainModel(memory, nBatch)\n",
    "        agent.updateStateStats(memory)\n",
    "        cEpoch = 0.\n",
    "        nSucc = 0\n",
    "        for eval_rep in range(1, nEval+1):\n",
    "            c, steps = evaluate(agent, eval_env, memory, \n",
    "                                render and eval_rep==nEval and \n",
    "                                epoch % renderStep == 0)\n",
    "            cEpoch += c\n",
    "            if steps == eval_env.max_steps - 1:\n",
    "                nSucc += 1\n",
    "\n",
    "        cost[epoch:] = cEpoch / nEval\n",
    "        idx = max(epoch - 50, 0)\n",
    "        idxEnd = max(epoch, 50)\n",
    "        cMean[idx:] = cost[idx:idxEnd].mean()\n",
    "        std = cost[idx:idxEnd].std()\n",
    "        cStdU[idx:] = cMean[idx] + std\n",
    "        cStdL[idx:] = cMean[idx] - std\n",
    "        evalSucc[epoch:] = nSucc\n",
    "        Qmin[epoch:] = Qs[0]\n",
    "        Qmax[epoch:] = Qs[1]\n",
    "        loss[epoch:] = l\n",
    "\n",
    "        with cEndWidget.batch_update():\n",
    "            lineCost.y = cost\n",
    "            lineCostM.y = cMean\n",
    "            lineCostSU.y = cStdU\n",
    "            lineCostSL.y = cStdL\n",
    "            lineEval.y = evalSucc\n",
    "            lineQmin.y = Qmin\n",
    "            lineQmax.y = Qmax\n",
    "            lineLoss.y = loss\n",
    "            cEndWidget.layout['title'] = \"Epoch: {}\".format(epoch)\n",
    "            if epoch % shapStep == 0:\n",
    "                # load training data from memory\n",
    "                data = Transition(*zip(*memory.sample(shapSamples)))\n",
    "                data = torch.cat(data.state).reshape(-1, agent.stateSize)\n",
    "                # scale data for NN\n",
    "                data = (data / agent.std).numpy()\n",
    "                # init explainer\n",
    "                explainer = shap.KernelExplainer(callNN, np.zeros((1, 4)), \n",
    "                                                 output_names=[\"push left\", \n",
    "                                                               \"push right\"])\n",
    "                # calculate shap values\n",
    "                shap_values = np.abs(explainer.shap_values(\n",
    "                                data, nsample=100, silent=True)).mean(1)\n",
    "                sPositionL[sIdx:] = shap_values[0][0]\n",
    "                sVelocityL[sIdx:] = shap_values[0][1]\n",
    "                sAngleL[sIdx:] = shap_values[0][2]\n",
    "                sTipVelocityL[sIdx:] = shap_values[0][3]\n",
    "                sPositionR[sIdx:] = shap_values[1][0]\n",
    "                sVelocityR[sIdx:] = shap_values[1][1]\n",
    "                sAngleR[sIdx:] = shap_values[1][2]\n",
    "                sTipVelocityR[sIdx:] = shap_values[1][3]\n",
    "                line_sPositionL.y = sPositionL\n",
    "                line_sVelocityL.y = sVelocityL\n",
    "                line_sAngleL.y = sAngleL\n",
    "                line_sTipVelocityL.y = sTipVelocityL\n",
    "                line_sPositionR.y = sPositionR\n",
    "                line_sVelocityR.y = sVelocityR\n",
    "                line_sAngleR.y = sAngleR\n",
    "                line_sTipVelocityR.y = sTipVelocityR\n",
    "                sIdx += 1\n",
    "\n",
    "        # early training stop if currently no convergence is visible\n",
    "        if epoch >= 0.5*nEpoch:\n",
    "            if cMean[idx] > 0.5*cMax and nSucc == 0:\n",
    "                for param in agent.model.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "                break\n",
    "\n",
    "        # stop training if controller can reproducible hold pole\n",
    "        if nSucc == nEval:\n",
    "            train = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.close()\n",
    "eval_env.close()\n",
    "# agent.loadModel()\n",
    "demo_env = CartPoleRegulatorEnv(cMax, mode=\"demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Epsilon = 0.\n",
    "obs = demo_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    aIdx = agent.act(obs)\n",
    "    obs, _, done, _ = demo_env.step(aIdx)\n",
    "    demo_env.render()\n",
    "demo_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "de5673f08cc042fd69b2464ab01c30abd62c4f4f16485517573956c9832f4ccd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
